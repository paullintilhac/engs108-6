{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yakaboskic/ENGS_108_Fall_2020/blob/master/assign_6_ENGS_108_Fall_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BiHNk7HcRiL"
   },
   "source": [
    "# **ENGS 108 Fall 2021 Assignment 6**\n",
    "\n",
    "*Due October 23 2021\n",
    "\n",
    "**Instructors:** George Cybenko\n",
    "\n",
    "**TAs:** Clement Nyanhongo, Jack Sadoff\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Rules and Requirements**\n",
    "\n",
    "\n",
    "1.   You are only allowed to use Python packages that are explicity imported in \n",
    "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
    "  *   [numpy](https://numpy.org/doc/stable/)\n",
    "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
    "  *   [matplotlib](https://matplotlib.org/)\n",
    "  *   [tensorflow](https://www.tensorflow.org/)\n",
    "\n",
    "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
    "\n",
    "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_KqjPtWlcFnS"
   },
   "outputs": [],
   "source": [
    "''' Import Statements '''\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "# Don't mess with this (gives reproducible results)\n",
    "random.seed(444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0otRQ4_CcxP4"
   },
   "source": [
    "## **Problem 1: Reinforcement Learning**\n",
    "In this problem we will play a game of cops and robbers. The game is played on a fixed undirected, simple, and finite graph $G$. There are two players, a cop and a robber. It is the goal of the cop to catch the robber in as few moves as possible. \n",
    "\n",
    "The graph $G$ has the following properties:\n",
    "  - A total of $m$ nodes.\n",
    "  - It contains a single $n$ node cycle, where $n\\leq m$, and random additional edges to make the graph connected.\n",
    "\n",
    "The game starts, with the cop taking their choice of vertex in $G$ and then the robber selects a random vertex in $G$ that is not occupied by the cop. At every point in the game both players know the positions of each other, and in this version of the problem we will say that the robber is drunk (i.e. they will randomly choose there next that instead of employing a policy).\n",
    "\n",
    "The availabe actions of the cop and associated reward function is:\n",
    "  - Move to a node not connected to their present node (and the cop stays in the current position): -5.\n",
    "  - Move to an adjacent node (including staying at current node): -1.\n",
    "  - Move to the node occupied by robber: +100.\n",
    ">\n",
    "> **Part 1** Building a Graph Class.\n",
    ">> **(a)** Using the provided skeleton build a general graph class for this problem for $n$ nodes. You are expected to implement *add_edge*, *make_random_graph*, *check_connected*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "jaHjFProBqKc"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Graph:\n",
    "  \"\"\" Our graph class.\n",
    "  Args:\n",
    "    n: Number of nodes in our cycle.\n",
    "    m: Total number of nodes in graph, where n >= m.\n",
    "  \"\"\"\n",
    "  def __init__(self, n, m):\n",
    "        \n",
    "    if (n > m):\n",
    "        raise Exception(\"n <= m must be true\")\n",
    "    self.n = n\n",
    "    self.m = m\n",
    "    \n",
    "    # A default dict is just a dict that won't raise a KeyError, it instead fills\n",
    "    # the unknown key with a default, in our case an empty list.\n",
    "    self.G = collections.defaultdict(list)\n",
    "\n",
    "  def add_edge(self, u, v):\n",
    "    \"\"\"\n",
    "    Make a function that will add an edge to the graph.\n",
    "    \"\"\"\n",
    "    if (u>self.m-1 or v>self.m-1):\n",
    "        raise Exception(\"node ids for new edge must both be less than \" + str(self.m))\n",
    "\n",
    "    self.G[u].append(v)\n",
    "\n",
    "\n",
    "  def make_random_graph(self):\n",
    "    \"\"\" First make a cycle of given length and then add random additional edges\n",
    "    in such a way that the final graph will be connected.\n",
    "    \"\"\"\n",
    "    #TODO: Make n length cycle first\n",
    "    count=0\n",
    "    #make a chain of the first n \n",
    "    prevIndex = random.choice(range(self.m))\n",
    "    #print(\"prevIndex: \" + str(prevIndex))\n",
    "    remaining_choices = range(self.m)\n",
    "    #print(\"remaining choices: \"+str(remaining_choices))\n",
    "    remaining_choices.remove(prevIndex)\n",
    "    #print(\"remaining choices: \"+str(remaining_choices))\n",
    "\n",
    "    originalIndex=prevIndex\n",
    "    for i in range(self.n-1):\n",
    "        nextIndex = random.choice(remaining_choices)\n",
    "        self.G[prevIndex].append(nextIndex)\n",
    "        prevIndex = nextIndex\n",
    "        remaining_choices.remove(prevIndex)\n",
    "        #print(\"remaining choices: \"+str(remaining_choices))\n",
    "\n",
    "    #connect head and tail of chain\n",
    "    self.G[nextIndex].append(originalIndex)\n",
    "    \n",
    "    \n",
    "    #TODO: Add additional nodes until random graph is connected\n",
    "    while True:\n",
    "      _graph = copy.copy(self)\n",
    "      uNew = random.randint(0,self.m-1)\n",
    "      vNew = random.randint(0,self.m-1)\n",
    "      _graph.G[uNew].append(vNew) if vNew not in _graph.G[uNew] else _graph.G[uNew]\n",
    "\n",
    "      if self.check_connected(_graph):\n",
    "        # If it is set the current graph's adjacency matrix equal to the copy's.\n",
    "        self.graph = _graph.G\n",
    "        # Return (i.e. break the loop)\n",
    "        return True\n",
    "\n",
    "  def check_connected(self, G):\n",
    "    \"\"\" Perform a Depth First Search.\n",
    "    \"\"\"\n",
    "    start_node = random.choice(list(G.G.keys()))\n",
    "    return dfs(G, [], start_node)\n",
    "\n",
    "#note that \"parent\" param is not strictly necessary\n",
    "#as long as we check whether a child node is already\n",
    "#in the listed of visited nodes before calling the \n",
    "#recursive function on it\n",
    "\n",
    "def dfs(G, visited, u):\n",
    "    \"\"\" Implement your own depth first search. Many online resources for this.\n",
    "    Args:\n",
    "    G: Is the graph (adjency matrix) we want to test.\n",
    "    visited: Is a dictionary that keeps track of the nodes you've visited.\n",
    "    u: a starting node.\n",
    "    Returns:\n",
    "    True: If all nodes have been visited at least once.\n",
    "    False: Otherwise.\n",
    "    \"\"\"\n",
    "    adjacent_nodes = G.G[u]\n",
    "    for node in adjacent_nodes:\n",
    "        if node not in visited:\n",
    "            visited.append(node)\n",
    "            dfs(G,visited,node)\n",
    "    if len(visited)==G.m:\n",
    "        return(True)\n",
    "    else: return(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc44XK7_JEYO"
   },
   "source": [
    ">> **(b)** Test out your graph class with a cops and robbers graph of $n=5$ and $m=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "pKxMuq_7BkGP"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb6d19d710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = Graph(5,10)\n",
    "g.make_random_graph()\n",
    "\n",
    "plotGraph = nx.DiGraph() \n",
    "\n",
    "for i in range(g.m): \n",
    " for j in range( g.m): \n",
    "   if j in g.G[i]: \n",
    "      plotGraph.add_edge(i,j) \n",
    "nx.draw( plotGraph) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTuchWzJJaxk"
   },
   "source": [
    "> **Part 2** Understanding the state space, i.e. the Game.\n",
    ">> **(a)** Given the graph class you've created in Part 1. Develop a Cops and Robbers game class. Use the skeleton below to implement the following functions first: *get_successors*, *terminal_test*, *result*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "5qBoFrNDBT9y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CopsAndRobbers:\n",
    "  def __init__(self, graph, start_state, rewards_table=None):\n",
    "    \"\"\" This is the cops and robbers game class.\n",
    "    Args:\n",
    "      start_state: The starting state of the cop position and robber position in\n",
    "        the graph. Should be a tuple of form (cop_pos, rob_pos).\n",
    "      G: The graph adjacency matrix\n",
    "      state: current state in the game.\n",
    "      reward_table: The (state, actions) reward dictionary that you will eventually implement.\n",
    "    \"\"\"\n",
    "    self.graph = graph\n",
    "    self.G = graph.G\n",
    "    self.state = start_state\n",
    "    self.rewards_table = rewards_table\n",
    "\n",
    "  def terminal_test(self, state):\n",
    "    \"\"\"test for terminal condition\"\"\"\n",
    "    if (state[0]==state[1]): \n",
    "        #print(\"state[0]: \" + str(state[0]) + \", state[1]: \" + str(state[1]))\n",
    "        return(True)\n",
    "    else: return(False)\n",
    "    \n",
    "  def get_successors(self, state):\n",
    "    \"\"\" Return a list of successor states that can be reached from the current state.\n",
    "    Hint: Remember only the cop can choose their action.\n",
    "    \"\"\"\n",
    "    copNextStates = self.G[state[0]]\n",
    "    #cop should be able to stay still\n",
    "    copNextStates.append(state[0]) if state[0] not in copNextStates else copNextStates\n",
    "    \n",
    "    successorStates = []\n",
    "    for i in copNextStates:\n",
    "        successorStates.append((i,state[1]))\n",
    "    return(successorStates)\n",
    "\n",
    "  def get_successors_robber(self, state):\n",
    "    \"\"\" Return a list of successor states that can be reached from the current state after the robber moves.\n",
    "    \"\"\"\n",
    "    robNextStates = self.G[state[1]]\n",
    "    #robber should be able to stay still\n",
    "    robNextStates.append(state[1]) if state[1] not in robNextStates else robNextStates\n",
    "    \n",
    "    successorStates = []\n",
    "    for i in robNextStates:\n",
    "        successorStates.append((state[0],i))\n",
    "    return(successorStates)\n",
    "\n",
    "  def result(self, next_state):\n",
    "    \"\"\" This function should return the state after the cop has made their move,\n",
    "    and the drunk robber has moved accordingly.\n",
    "    Args:\n",
    "      state: Current state of (cop, rob).\n",
    "      next_state: The state after the cop has moved (next_cop, rob). Calculated from get_successors.\n",
    "    \"\"\"\n",
    "    \n",
    "    successorStates = self.get_successors_robber(next_state)\n",
    "    #print(\"robber successor states: \" + str(successorStates))\n",
    "    final_state = random.choice(successorStates)\n",
    "    #print(\"robber moved from \" + str(next_state) + \" to \" + str(final_state))\n",
    "    return(final_state)\n",
    "\n",
    "  #I'm not going to use this function. Because in order for it to be useful, this should \n",
    "  # be based on the calculate_rewaard_table method, which is not defined yet. So unless I\n",
    "  #re-arranged the questions, I would have to re-define this class again using that function!\n",
    "  def utility(self, state, action):\n",
    "    return self.rewards_table[(state, action)]\n",
    "\n",
    "CR = CopsAndRobbers(g,(0,1))\n",
    "CR.get_successors(CR.state)\n",
    "CR.terminal_test((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-YjCqNsXOKG"
   },
   "source": [
    ">> **(b)** In reinforcement learning we are often interested in calculating a rewards table that has possible states as its rows and possible actions as its columns and filled in with the associated reward given the Q(state, action) pair. Calculate the rewards table for any given graph. *Hint: This should be an $m^2$ x $m$ matrix or a dictionary with $m^3$ keys such that the keys are (state, action) tuples.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "F61FjlM__wEg"
   },
   "outputs": [],
   "source": [
    "def calculateRewardsTable(graph):\n",
    "  \"\"\" Make a rewards table dictionary of the from table[(state, action)] = reward. recall:\n",
    "  The availabe actions of the cop and associated reward function is:\n",
    "\n",
    "  Move to a node not connected to their present node (and the cop stays in the current position): -5.\n",
    "  Move to an adjacent node (including staying at current node): -1.\n",
    "  Move to the node occupied by robber: +100.\n",
    "  \"\"\"\n",
    "  table = collections.defaultdict(list)\n",
    "  for c_state_ind in range(graph.m):\n",
    "    for r_state_ind in range(graph.m):\n",
    "        for action in range(graph.m):\n",
    "            reward = 0\n",
    "            cop_robber = (c_state_ind,r_state_ind)\n",
    "            if action in graph.G[c_state_ind]:\n",
    "                if (action==r_state_ind):\n",
    "                    reward=100\n",
    "                else: reward=-1\n",
    "            else:\n",
    "                if (c_state_ind==r_state_ind):\n",
    "                    reward=100\n",
    "                else: reward = -5\n",
    "            table[(cop_robber,action)]=reward\n",
    "  return table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU0Hq2C8bocS"
   },
   "source": [
    ">> **(c)** Now that we have our reward table, try to solve the problem in a brute-force manner (without reinforcement learning). I.e. try to reach the terminal state, or find get a reward of 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "GysY6hjZ_Iu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {0: [2, 6, 0, 7], 1: [6, 1], 2: [1], 3: [9], 4: [0, 6], 5: [6], 6: [4, 5, 3], 7: [3], 8: [8, 1], 9: [6]})\n",
      "epoch #1\n",
      "previousState: (1, 8)\n",
      "reward: -5\n",
      "attemptedNextState: (9, 8)\n",
      "cop successors: [(6, 8), (1, 8)]\n",
      "attempted to move cop from 1 to non-adjacent node 9\n",
      "next state: (1, 8)\n",
      "epoch #2\n",
      "previousState: (1, 8)\n",
      "reward: -5\n",
      "attemptedNextState: (0, 8)\n",
      "cop successors: [(6, 8), (1, 8)]\n",
      "attempted to move cop from 1 to non-adjacent node 0\n",
      "next state: (1, 8)\n",
      "epoch #3\n",
      "previousState: (1, 8)\n",
      "reward: -5\n",
      "attemptedNextState: (0, 8)\n",
      "cop successors: [(6, 8), (1, 8)]\n",
      "attempted to move cop from 1 to non-adjacent node 0\n",
      "next state: (1, 8)\n",
      "epoch #4\n",
      "previousState: (1, 8)\n",
      "reward: -5\n",
      "attemptedNextState: (3, 8)\n",
      "cop successors: [(6, 8), (1, 8)]\n",
      "attempted to move cop from 1 to non-adjacent node 3\n",
      "next state: (1, 8)\n",
      "epoch #5\n",
      "previousState: (1, 8)\n",
      "reward: 100\n",
      "attemptedNextState: (7, 1)\n",
      "cop successors: [(6, 8), (1, 8)]\n",
      "attempted to move cop from 1 to non-adjacent node 7\n",
      "next state: (1, 1)\n",
      "rewards: 80, # epochs to terminate: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cr = CopsAndRobbers(g,(1,8))\n",
    "def simulate_random(cr_temp,verbose=False):\n",
    "# Simulation loop\n",
    "    epochs=0\n",
    "    rewards = []\n",
    "    penalties = 0\n",
    "    rTable = calculateRewardsTable(cr_temp.graph)\n",
    "\n",
    "    while True:\n",
    "        epochs=epochs+1\n",
    "        \n",
    "        previousState = cr_temp.state\n",
    "        if verbose:\n",
    "            print(\"epoch #\" + str(epochs))\n",
    "            print(\"previousState: \" + str(previousState))\n",
    "        attemptedNextCopState = random.choice(range(cr_temp.graph.m))\n",
    "        \n",
    "        nextStateWithoutRobberMoving = (attemptedNextCopState,previousState[1])\n",
    "        #optimistically set next cop state\n",
    "        #if cop moved to non-adjacent node, reset their position\n",
    "        attemptedNextState = cr_temp.result(nextStateWithoutRobberMoving)\n",
    "        #we need attemptedNextStateWithoutCopMoving in order to calculate the appropriate reward \n",
    "        #after the cop's action calculate reward based on previous state of cops and robbers and new state of cop\n",
    "        attemptedNextStateWithoutCopMoving = (previousState[0],attemptedNextState[1])\n",
    "        \n",
    "        # in order to calculate reward based on both the cop's action and the \n",
    "        # robber's simultaneous random reaction, we take reward((old_cop_state,new_robber_state),new_cop_state)\n",
    "        reward = rTable[attemptedNextStateWithoutCopMoving,attemptedNextState[0]]\n",
    "        if verbose:\n",
    "            print(\"reward: \" + str(reward))\n",
    "            print(\"attemptedNextState: \" + str(attemptedNextState))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # note that we could just as well skip the getSuccessors functionality entirely,\n",
    "        # and just check whether the reward is 0 before re-setting. but I opted to use that function.\n",
    "        \n",
    "        # get successor states where robber is the same as current (robber hasn't moved yet)\n",
    "        cSuccessors = cr_temp.get_successors(previousState)\n",
    "        if verbose:\n",
    "            print(\"cop successors: \" + str(cSuccessors))\n",
    "        nextState = attemptedNextState\n",
    "        if (attemptedNextCopState,previousState[1]) not in cSuccessors:\n",
    "            allowedCopTransition = True\n",
    "            nextState = (previousState[0],nextState[1])\n",
    "            if verbose:\n",
    "                print(\"attempted to move cop from \" + str(previousState[0]) + \" to non-adjacent node \" + str(attemptedNextCopState))\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"moved cop from \" + str(previousState[0]) + \" to adjacent node \" + str(attemptedNextCopState))\n",
    "\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"next state: \" + str(nextState))\n",
    "        cr_temp.state = nextState\n",
    "        \n",
    "        if (cr_temp.terminal_test(nextState)):\n",
    "            break\n",
    "    return(rewards,epochs)\n",
    "\n",
    "rewards,term = simulate_random(cr,verbose=True)\n",
    "print(\"rewards: \" + str(sum(rewards)) + \", # epochs to terminate: \" + str(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec8eFd77jdiV"
   },
   "source": [
    "> **Part 3** Q-learning. \n",
    ">\n",
    "> Up to this point we have build a graph class, built a game class, ran a brute-force simulation of an agent traversing the space randomly, and now we will dive into Q-learning in the hopes of maximizing the rewards and efficiency of capturing the drunk robber. \n",
    ">\n",
    ">> **(a)** Using the skeleton from the brute force method, implement a training loop to learn a Q-table for a given graph and game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "FhpxYwKn-rIa"
   },
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self,graph,alpha=.1,gamma=.1):\n",
    "        self.n_states = graph.m\n",
    "        self.RTable = calculateRewardsTable(graph)\n",
    "        #initialize q table\n",
    "        table = collections.defaultdict(list)\n",
    "        for c_state_ind in range(graph.m):\n",
    "            for r_state_ind in range(graph.m):\n",
    "                for action in range(graph.m):\n",
    "                    cop_robber = (c_state_ind,r_state_ind)\n",
    "                    table[(cop_robber,action)]=0\n",
    "        self.QTable = table\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def setQTable(self,state,action,value):\n",
    "        #print(\"setting qtable value for state \" + str(state) + \" and action \" + str(action) + \" to \" + str(value))\n",
    "        self.QTable[state,action]=value\n",
    "\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FVu7FlNk3IA"
   },
   "source": [
    ">> **(b)** Evalute your new Q-learning agent over a 100 epochs, by choosing your actions based on the argmax of the Q-table caluculated in (a) and report the average number of penalities, average time, and average number of steps it took to find the robber with your new Q-learning strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "zqnDZQ3C-1qE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevIndex: 7\n",
      "remaining choices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "remaining choices: [0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "remaining choices: [0, 2, 3, 4, 5, 6, 8, 9]\n",
      "remaining choices: [2, 3, 4, 5, 6, 8, 9]\n",
      "remaining choices: [3, 4, 5, 6, 8, 9]\n",
      "remaining choices: [3, 4, 5, 6, 8]\n",
      "defaultdict(<type 'list'>, {0: [2, 0], 1: [0, 7, 3], 2: [9, 7, 4, 8, 6], 3: [2, 5], 4: [3, 8, 2], 5: [], 6: [9], 7: [1, 2, 4], 8: [7], 9: [7, 8, 0, 1]})\n"
     ]
    }
   ],
   "source": [
    "g2 = Graph(5,10)\n",
    "g2.make_random_graph()\n",
    "print(str(g2.G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #1\n",
      "previous state: (0, 1)\n",
      "best action: 0\n",
      "reward: -1\n",
      "cop successors: [(2, 1), (0, 1)]\n",
      "next state: (0, 7)\n",
      "moved cop from 0 to adjacent node 0\n",
      "setting new q value for state (0, 1) and action 0: -0.5\n",
      "epoch #2\n",
      "previous state: (0, 1)\n",
      "best action: 1\n",
      "reward: -5\n",
      "cop successors: [(2, 1), (0, 1)]\n",
      "next state: (1, 3)\n",
      "attempted to move cop from 0 to non-adjacent node 1\n",
      "setting new q value for state (0, 1) and action 0: -2.5\n",
      "epoch #3\n",
      "previous state: (0, 1)\n",
      "best action: 1\n",
      "reward: 100\n",
      "cop successors: [(2, 1), (0, 1)]\n",
      "next state: (1, 0)\n",
      "attempted to move cop from 0 to non-adjacent node 1\n",
      "setting new q value for state (0, 1) and action 0: 50.0\n",
      "rewards: [-1, -5, 100], epochs to terminate: 3\n"
     ]
    }
   ],
   "source": [
    "cr2 = CopsAndRobbers(g2,(0,1))\n",
    "def QLearningSim(cr_temp,verbose=False):\n",
    "    epochs = 0\n",
    "    rewards = []\n",
    "    penalties = 0\n",
    "    rTable = calculateRewardsTable(cr_temp.graph)\n",
    "    qTable= QTable(cr_temp.graph,alpha=.5,gamma=.5)\n",
    "    # Simulation loop\n",
    "    while True:\n",
    "        epochs=epochs+1\n",
    "        if verbose:\n",
    "            print(\"epoch #\" + str(epochs))\n",
    "        previousState = cr_temp.state\n",
    "\n",
    "        if verbose:\n",
    "            print(\"previous state: \" + str(previousState))\n",
    "        bestQVal = -10000\n",
    "        bestAction = None\n",
    "        for i in range(cr_temp.graph.m):\n",
    "            qVal = qTable.QTable[previousState,i]\n",
    "            if qVal>bestQVal:\n",
    "                bestQVal = qVal\n",
    "                bestAction = i\n",
    "        if verbose:\n",
    "            print(\"best action: \" + str(bestAction))\n",
    "\n",
    "        nextStateWithoutRobberMoving = (bestAction,previousState[1])\n",
    "        attemptedNextState = cr_temp.result(nextStateWithoutRobberMoving)\n",
    "        attemptedNextStateWithoutCopMoving = (previousState[0],attemptedNextState[1])\n",
    "        reward = rTable[attemptedNextStateWithoutCopMoving,attemptedNextState[0]]    \n",
    "        if verbose:\n",
    "            print(\"reward: \" + str(reward))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        cSuccessors = cr_temp.get_successors(previousState)\n",
    "        nextState = attemptedNextState\n",
    "        if verbose:\n",
    "            print(\"cop successors: \" + str(cSuccessors))\n",
    "            print(\"next state: \" + str(nextState))\n",
    "        if (bestAction,previousState[1]) not in cSuccessors:\n",
    "            if verbose:\n",
    "                print(\"attempted to move cop from \" + str(previousState[0]) + \" to non-adjacent node \" + str(bestAction))\n",
    "            nextState = (previousState[0],nextState[1])\n",
    "        else: \n",
    "            if verbose: \n",
    "                print(\"moved cop from \" + str(previousState[0]) + \" to adjacent node \" + str(bestAction))\n",
    "\n",
    "\n",
    "        oldQVal = qTable.QTable[previousState,bestAction]\n",
    "\n",
    "        bestAction = None\n",
    "        highestQ = -1000000\n",
    "\n",
    "        for thisAction in range(cr_temp.graph.m):\n",
    "            thisQ = qTable.QTable[nextState,thisAction]\n",
    "            if thisQ>highestQ:\n",
    "                highestQ = thisQ\n",
    "                bestAction = thisAction\n",
    "\n",
    "        #print(\"self.alpha: \"+str(qTable.alpha)+ \", oldQVal: \" + str(oldQVal) + \", reward: \" + str(reward) + \", self.gamma: \" + str(qTable.gamma) + \", highestQ: \" + str(highestQ))\n",
    "        newQVal = (1-qTable.alpha)*oldQVal+qTable.alpha*(reward+qTable.gamma*highestQ)\n",
    "        if verbose:\n",
    "            print(\"setting new q value for state \"+str(previousState)+\" and action \" + str(bestAction) +\": \" + str(newQVal))    \n",
    "        qTable.setQTable(previousState,bestAction,newQVal)\n",
    "        if (cr_temp.terminal_test(nextState)):\n",
    "            break\n",
    "    return(rewards,epochs)\n",
    "rewards, term = QLearningSim(cr2,verbose=True)\n",
    "print(\"rewards: \" + str(rewards) + \", epochs to terminate: \" + str(term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrpB9jKolvH_"
   },
   "source": [
    ">> **(c)** Compare your results with the brute-force method used in Part 2 and comment on the improvement. For instance, try varying graph configurations and look for any signs of improvement in certian instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance depends not only on the shape and size of the graph, but also very crucially on the initial positions of the cop and the robber. In order to really test which one is performing better, we should iterate over several random initial positions, running both algorithms on each, and then summarize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "l6b7G2mB_FIr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean total rewards for random sim over 100 simulations: 74.62\n",
      "mean total rewards for q sim over 100 simulations: 93.08\n",
      "mean term for random sim over 100 simulations: 8.9\n",
      "mean term for q sim over 100 simulations: 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def run_tests(n=4,n_sim=100):\n",
    "\n",
    "    test_graph = Graph(n,8)\n",
    "    test_graph.make_random_graph()\n",
    "    #print(str(test_graph.G))\n",
    "    \n",
    "    total_rewards_rand = []\n",
    "    terms_rand = []\n",
    "    total_rewards_q = []\n",
    "    terms_q = []\n",
    "\n",
    "    for i in range(n_sim):\n",
    "        initialCop = random.choice(range(test_graph.m))\n",
    "        newList = range(test_graph.m)\n",
    "        #print(\"newList: \" + str(newList))\n",
    "        newList.remove(initialCop)\n",
    "        #print(\"newList2: \" + str(newList))\n",
    "\n",
    "        initialRobber = random.choice(newList)\n",
    "        \n",
    "        #print(\"initial cop: \" + str(initialCop) + \", initialRobber: \" + str(initialRobber))\n",
    "        #cr3 = CopsAndRobbers(test_graph,(initialCop,initialRobber),rTable2)\n",
    "        cr3 = CopsAndRobbers(test_graph,(initialCop,initialRobber))\n",
    "\n",
    "        random_sim_rewards,random_sim_term = simulate_random(cr3,verbose=False)\n",
    "        total_rewards_rand.append(sum(random_sim_rewards))\n",
    "        terms_rand.append(random_sim_term)\n",
    "        #print(\"random sim total reward: \" + str(sum(random_sim_rewards)) + \", random sim epochs: \" + str(random_sim_term))\n",
    "        q_sim_rewards,q_sim_term = QLearningSim(cr3,verbose = False)\n",
    "        total_rewards_q.append(sum(q_sim_rewards))\n",
    "        terms_q.append(q_sim_term)\n",
    "        #print(\"q sim total reward: \" + str(sum(q_sim_rewards)) + \", q sim epochs: \" + str(q_sim_term))\n",
    "\n",
    "    \n",
    "    print(\"mean total rewards for random sim over \"+str(n_sim) +\" simulations: \" + str(np.mean(total_rewards_rand)))\n",
    "    print(\"mean total rewards for q sim over \"+str(n_sim) +\" simulations: \" + str(np.mean(total_rewards_q)))\n",
    "    print(\"mean term for random sim over \"+str(n_sim) +\" simulations: \"  + str(np.mean(terms_rand)))\n",
    "    print(\"mean term for q sim over \"+str(n_sim) +\" simulations: \"  + str(np.mean(terms_q)))\n",
    "\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears that the q-learning version is much better than brute force search. At first when I saw these results, I thought maybe that there was a bias towards the Q-Learning algorithm that was not very generalizable, because of the way my cycles were being generated. Because of the way the Q-learning algorithm takes the argmax of the Q values as its action, which are initially all 0, this means that it will try each action in order, at least at the beginning of the algorithm. So if the cycle is also structured in a way that correlates with the node indices because of the way it is constructed, this could create a bias for the q-learning algorithm that is not real (because we should be able to permute the indices without changing the general results). \n",
    "But then I made sure to choose random indices when creating that cycle initially, and the same overall result remained, which was that the q-learning algorithm did much better. It had much higher rewards, and converged much quicker. \n",
    "To check for how it depends on different raph configurations, I looked at a graph where the total number of nodes was the same, but the cycle size was very large, or very small. Below are the results for (m=8,n=6), and (m=8,n=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean total rewards for random sim over 100 simulations: 41.88\n",
      "mean total rewards for q sim over 100 simulations: 97.71\n",
      "mean term for random sim over 100 simulations: 17.8\n",
      "mean term for q sim over 100 simulations: 2.29\n"
     ]
    }
   ],
   "source": [
    "run_tests(2,n_sim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement of q-learning over brute force method was more exaggerated for the case where the cycle was smaller relative to the graph. And the two strategies were closer when the cycle was larger relative to the graph. My interpretation of this is that if there is only a small cycle, then the cop should be able to have some concrete idea of how \"far\" they are from the robber, and therefore in many situations should be able to pick an action that moves the cop \"closer\" to the robber. Whereas if the graph is more cyclic, then there may be no way to make a move that guarantees the cop always gets closer to the robber."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDZuSjRS944s"
   },
   "source": [
    "> **Bonus** Check that the learned policy satsifies the [Bellman Inequality](https://towardsdatascience.com/mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f), i.e is the computed solution the actual optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BKvya8C_GJH"
   },
   "source": [
    "#TODO: Your code goes here.\n",
    "well, the solution depends on two parameters that I chose arbitrarily, gamma and alpha. So it is highly unlikely that this is truly the optimal policy. However, given a specific gamma and alpha, the policy satisfies the bellman inequality because the process is a markov decision process, which means that the optimal policy at a given state must have an expected utility equal to the expected utility from the current policy applied to the current state, plus the maximum expected utility at the next state. Since the policy is chosen as the highest value of Q for the given state, and the q table gets updated based on the best possible action from the subsequent state, this condition is satisfied according to our policy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMu25Ay7SN5ZWySgfPySRxZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1EXaYP-OTNO9eltRbwKEkUondRH_HxZgx",
   "name": "assign_6_ENGS_108_Fall_2020.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
